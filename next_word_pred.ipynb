{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658c161d",
   "metadata": {},
   "source": [
    "# Next-Word Prediction with LSTM\n",
    "\n",
    "This notebook demonstrates a simple next-word prediction pipeline using an LSTM-based language model (TensorFlow / Keras). The project uses a cleaned text file (`metamorphosis_clean.txt`) as the training corpus and shows the full flow: tokenization, sequence creation, padding, model definition, training, and inference (generating the next words).\n",
    "\n",
    "Sections in this notebook:\n",
    "- Imports and setup\n",
    "- Data loading and preprocessing\n",
    "- Building the LSTM model\n",
    "- Training and evaluation\n",
    "- Simple inference loop to generate next words\n",
    "\n",
    "Notes: replace `metamorphosis_clean.txt` with your own plain-text corpus if you want to train on a different dataset. For reproducible runs, pin TensorFlow to a compatible version (see README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd6023",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n",
    "\n",
    "We load the full text file into a single string, then use Keras' Tokenizer to build a word index. The notebook splits the text into lines and builds incremental n-gram sequences from each line (e.g. `[w1, w2] -> w3`, `[w1, w2, w3] -> w4`, ...). Sequences are padded to the same length so they can be batched.\n",
    "\n",
    "Key preprocessing steps:\n",
    "- Tokenize the text to integers (word -> index).\n",
    "- Build input sequences of increasing prefix lengths for next-word prediction.\n",
    "- Pad sequences to a fixed length with `pad_sequences` (pre-padding).\n",
    "- Split sequences into `X` (prefixes) and `y` (next word), then one-hot encode `y` with `to_categorical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metamorphosis_clean.txt\", \"r\") as f:\n",
    "    input_text = f.read()\n",
    "print(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([input_text])\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for sen in input_text.split('\\n'):\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sen])[0]\n",
    "    for i in range(1,len(tokenized_sentence)):\n",
    "        sequences.append(tokenized_sentence[:i+1])\n",
    "# print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(x) for x in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_len,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6738bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y,num_classes=len(tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08004176",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa9a06",
   "metadata": {},
   "source": [
    "## Model architecture and hyperparameters\n",
    "\n",
    "A straightforward LSTM language model is defined using Keras' Sequential API. The notebook uses an Embedding layer to learn dense word vectors, followed by a single LSTM layer and a Dense softmax output over the vocabulary.\n",
    "\n",
    "Things to consider and tune:\n",
    "- Embedding size (currently 100).\n",
    "- LSTM units (currently 200).\n",
    "- Vocabulary size (derived from tokenizer; the notebook hardcodes an example value in the model definition — replace with `len(tokenizer.word_index)+1` for general runs).\n",
    "- Input length (max sequence length) — used by the Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447afc7",
   "metadata": {},
   "source": [
    "## Training and notes\n",
    "\n",
    "The model is compiled with categorical crossentropy and trained for a modest number of epochs (example uses 50). For larger corpora or improved performance, consider adding callbacks (ModelCheckpoint, EarlyStopping), using a validation split, and experimenting with learning rates and optimizers.\n",
    "\n",
    "Training tips:\n",
    "- Save the model weights after the best validation accuracy.\n",
    "- Use batch sizes appropriate for your GPU/CPU memory.\n",
    "- For faster training, reduce the vocabulary (filter rare words) or use subword tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa207af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(2618,100, input_length=17))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(2618,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e053d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae5d8c",
   "metadata": {},
   "source": [
    "## Inference / Generating text\n",
    "\n",
    "A small example loop below demonstrates how to seed the model with a short phrase and iteratively predict the next word. Notes:\n",
    "- The notebook uses argmax on the softmax output which picks the single most likely word; sampling from the distribution (temperature sampling) can produce more varied, creative outputs.\n",
    "- Make sure to preprocess the seed text the same way as training (tokenization and padding).\n",
    "- Save and reload both the trained model and the tokenizer for reproducible inference outside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e55688",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"random setting of the house\"\n",
    "for i in range(5):\n",
    "    token_text = tokenizer.texts_to_sequences([text])\n",
    "    print(token_text)\n",
    "    padded_seq = pad_sequences(token_text, maxlen=max_len,padding='pre')\n",
    "    pos = np.argmax(model.predict([padded_seq]))\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index==pos:\n",
    "            text = text + ' ' + word\n",
    "            print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad3a82",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The training loss stays low while validation loss rises. In simple terms: the model learns the training examples well but does worse on unseen data (overfitting).\n",
    "\n",
    "Here is how we can fix this:\n",
    "* Shuffle and split the data properly (keep a separate test set).\n",
    "* Use EarlyStopping and save the best model (ModelCheckpoint).\n",
    "* Regularize or reduce model size, and check example predictions by hand.\n",
    "* Gather **more training data** for better generalization.\n",
    "* Try **hyperparameter tuning** (embedding size, LSTM units, learning rate, dropout).\n",
    "* Explore **advanced architectures** such as stacked/bidirectional LSTMs or Transformer-based models.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
